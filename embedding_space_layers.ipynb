{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNlAYNI20fml+oQTIjV5q6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarahkaarina/embedding_space/blob/main/embedding_space_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computing narrative flow of information**\n",
        "\n",
        "This notebook demonstrated how word embedding similarity observationally changes across layers in two language models, GPT2 and BERT.\n",
        "\n",
        "*What do we mean by word embedding similarity?*\n",
        "\n",
        "**STEP 1:** For each word in a given narrative, we can extract a token level embedding that is then mapped to the word from every layer of a given language model (i.e., GPT-2). This results in a list of word embeddings, arranged from the first word in the narrative to the last. With this arrangement, we can now evalute word by word similarity along the temporal dimension of the narrative. In generative models like GPT, word embeddings contain not only information relating to the word itself but also information related to the context preceeding the word. Therefore, similarity between word embeddings should be sensitive to how meaningful/linguistic information flows through a given narrative.\n",
        "\n",
        "**STEP 2:** After extracting the word embeddings, from first to last, we can then compute cosine similarity between each of these word embeddings. This results in a word by word symmetric matrix containing the cosine similarity values, where the diagonal only contains values of 1 (perfect cosine similarity).\n",
        "\n",
        "**STEP 3:** We can then take the lower triangle of the word embedding matrix to get a 1-D vector that represents the distribution of word-by-word similarity values. Rounding these values to the first decimal place, gives us a discrete probability distribution of values -1 =< x =< 1. Using Shannon's entropy (below), we can use this probability distribution to estimate the noise in the signal, that is with what certainty words are completely similar or dissimilar to one another. We expect that in a given narrative, similarity distributions that contain word-level feature information should be very 'noisy' with a blend of words that are similar/dissimilar to each other on the basis of semantic content/syntatic features/distance apart. Instances where the model embedding space starts to converge onto higher level meanings should result in lower entropy values, such as in the upper layers of GPT-2 where we see anisotropic distributions of the embedding space, than when the embedding space is more isotropically distributed.\n",
        "\n",
        "![Screenshot 2025-05-26 at 2.56.24 PM.png](data:image/png;base64,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)\n",
        "\n"
      ],
      "metadata": {
        "id": "Wc_-Qz-smu7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Demo**\n",
        "\n",
        "To demonstrate how word embeddings relate to each other across a given contextual window and to show how entropy changes as a function of layers for the two models used in our paper (GPT-2 and BERT), I will load a dataset containing the plot summaries of movies from the HuggingFace datasets library.\n",
        "\n",
        "After computing entropy, I will plot the entropy values for every example narrative across the layers of these two models.\n",
        "\n",
        "Additionally, I will plot the similarity matrix itself, where I can examine how word embedding similarity is distributed for every layer in three examples narrative, and the t-SNE clustering results on two-dimensions for 10 example narratives. I will do this separately for each model.\n",
        "\n",
        "Data reference: https://huggingface.co/datasets/vishnupriyavr/wiki-movie-plots-with-summaries\n"
      ],
      "metadata": {
        "id": "CZoxbBdu9h89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HKXxtKb7RrM"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade datasets fsspec --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# datasets gave me a bunch of problems if I only installed !pip install datasets without upgrading my fsspec\n",
        "# infatti, you'll still see a bunch of error messages when you run the above cell\n",
        "# I ASSUME (assume) this is related to me using datasets in collab - if you test this code out yourself on a local machine\n",
        "# PLEASE CHECK THAT RUNNING UPGRADING FFSPEC DOESN'T MESS WITH YOUR LOCAL DEPENDENCIES\n",
        "#\n",
        "# Adding trust_remote_code=True allows for datasets to be imported in collab\n",
        "# not adding again generates some errors AGAIN assumptively from using collab and not a local machine\n",
        "# Therefore:\n",
        "# --upgrade datasets ffspec\n",
        "# trust_remote_code=True\n",
        "# Most likely depend on this script running in Collab and should be revaluted before being run on something local/not Collab\n",
        "\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "random.seed(999)\n",
        "\n",
        "ds = load_dataset(\"vishnupriyavr/wiki-movie-plots-with-summaries\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "D50JjdTz7bNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_summaries = ds[\"train\"][\"Plot\"]"
      ],
      "metadata": {
        "id": "2DLC-e6X8zL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# Create a translator (kind of like regex in R)\n",
        "# where we will remove all punctuation EXCEPT for apostrophes!\n",
        "# WHY? Without apostrophes, the GPT tokenizer (later on) will not understand\n",
        "# to embed pronous + auxiliaries\n",
        "# NB: FOR THIS ANALYSIS, I won't remove punctation because we are dealing with WRITTEN not ORAL text.\n",
        "# Therefore, punctuation is important as de-marks something about the organization of written data.\n",
        "# If I were working with transcribed oral language, I would remove punctuation as now punctuation becomes artefactual\n",
        "# interpreation on the part of the transcriber (may this be Open AI's Whisper or a real human) of how utterances are\n",
        "# constructed into written speech\n",
        "# (TLDR; full stops don't mean much for how information is used in spoken language, they just make it legible.).\n",
        "\n",
        "punctuation_except_apostrophe = string.punctuation.replace(\"'\", \"\")\n",
        "remove_punctuation = str.maketrans('', '', punctuation_except_apostrophe)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# library for removing substrings\n",
        "import re\n",
        "\n",
        "import math\n",
        "\n",
        "#libraries for calculating similarity\n",
        "#if you need to install sklearn as a module, it will only work if you install as scikit-learn\n",
        "#however, will still let you import through sklearn module\n",
        "\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# import your PRETRAINED OFFLINE MODELS\n",
        "# I will specify this in all caps - why?\n",
        "# From my understanding, data that is under non-commerical use license (i.e., most research data, even open source stuff)\n",
        "# cannot be passed to a language model that actively stores, fine-tunes, or trains on that data.\n",
        "# THEREFORE IF YOUR DATA IS UNDER NON-COMMERICAL USE\n",
        "# The most appropriate way (as I understood it) is to download the weights and tokenizers from pre-trained models using the transformers library\n",
        "# As long as the models are used offline (and you are respecting any data usage agreement), i.e., you are not interfacing with an API/commercial entity you can fine-tune, embed, etc. to your heart's\n",
        "# content on data restricted by non-commerical use licensing.\n",
        "# THIS IS MY UNDERSTANDING (I repeat)\n",
        "# If what I said sounds like complete bolony please feel free to tell me otherwise by raising an issue to this repo's git hub page\n",
        "# or stalking me online and finding my most current institutional email\n",
        "from transformers import GPT2Tokenizer, GPT2Model, BertTokenizer, BertModel\n",
        "\n",
        "import torch\n",
        "# Fun hack, if you have a Mac with an MP processor (not an Intel one) you can probably run this locally on the MP's GPU\n",
        "# But because it's a mac torch won't find a CUDA GPU but instead needs to look for an MPS one\n",
        "# have fun on my behalf (I still haven't upgraded my mac to the silicone processor): https://developer.apple.com/metal/pytorch/\n",
        "# NB: if like me you only have local CPUs this code will still run (as it will default to CPU)\n",
        "# BUT you may need to fix a few lines further down as when the tensor items are returned to the CPU they need to be converted from GPU readable\n",
        "# to cpu readable.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# set the device globally\n",
        "torch.set_default_device(device)\n",
        "\n",
        "# import Spacy tokenizer\n",
        "#\n",
        "# Spacy is an NLP tools that will tag parts of speech, so tokenizes at a word and morpheme level\n",
        "# (i.e., I [pronoun] am [auxiliary] and I [pronoun]'m [auxiliary])\n",
        "# Spacy is trained specific to language, that is, while a LLM tokenizer simply cares about frequently occurent 'bits'\n",
        "# Spacy wants to look for parts of language that confirm with predetermined notions of word-class/syntactic rules\n",
        "# for reference: https://spacy.io/api/tokenizer\n",
        "# I would also reccomend this lecture, for a breakdown of how LLM (i.e. BPE tokenizers) work: https://www.youtube.com/watch?v=9vM4p9NN0Ts&t=541s\n",
        "#\n",
        "# NB. I am not an NLP person, but I wanted to flag this difference because when you have typos (as may happen in transcript/medical record data)\n",
        "# the Spacy tokenizer is clever in that if it can identify something that looks like a pronoun ('Im' written instead of 'I'm) it will still tag\n",
        "# I [pronoun] and m [auxiliary] correctly. A BPE (like the one used by GPT) will not, I will see 'Im' as one token.\n",
        "# Indeed, this won't be the case for the data in this notebook but if you part of speech tag real data (with typos) and then try to line it up with token\n",
        "# level distinctions in GPT2, the two may not line up.\n",
        "#\n",
        "# Just something to consider.\n",
        "\n",
        "import spacy\n",
        "\n",
        "# what you need to download the images from this notebook\n",
        "# NB: I will leave the downloading lines of code commented out so that if you run it your downloads folder won't clog up with stuff.\n",
        "# Uncomment to download said images.\n",
        "from google.colab import files\n",
        "\n",
        "import random\n"
      ],
      "metadata": {
        "id": "BBAfccUe--c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(narrative):\n",
        "    spaces = narrative.count(' ')\n",
        "    tabs = narrative.count('\\t')\n",
        "    newlines = narrative.count('\\n')\n",
        "    words = spaces+tabs+newlines+1\n",
        "    return words"
      ],
      "metadata": {
        "id": "7JMvK39l_vRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_word_embedding(sentences2embed, lm_model: str):\n",
        "\n",
        "    if lm_model =='gpt':\n",
        "        model_name = 'gpt2'\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        model = GPT2Model.from_pretrained(model_name)\n",
        "\n",
        "    elif lm_model =='bert':\n",
        "        tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
        "        model = BertModel.from_pretrained(\"google-bert/bert-base-cased\")\n",
        "\n",
        "    # Run model on GPU\n",
        "    model = model.to(device)\n",
        "    sentences2embed = str(sentences2embed)\n",
        "\n",
        "    # Tokenize and pad sequences\n",
        "    encoded_sentences = tokenizer(\n",
        "                  sentences2embed,\n",
        "                  return_tensors='pt')\n",
        "\n",
        "    inputs = encoded_sentences['input_ids']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs.to(device), output_hidden_states=True)\n",
        "    hidden_states = outputs.hidden_states\n",
        "\n",
        "    return hidden_states\n",
        "\n",
        "def tag_words(sentences2embed):\n",
        "\n",
        "    # load parts of speech tagger\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    doc = nlp(sentences2embed)\n",
        "    word_classes = [(token.text, token.pos_) for token in doc]\n",
        "    return word_classes\n",
        "\n",
        "def grab_embeddings_by_layer(hidden_states, layer, word_classes):\n",
        "    layer = int(layer)\n",
        "    layer_embeddings = hidden_states[layer]\n",
        "    embeddings = layer_embeddings.squeeze()\n",
        "\n",
        "    # Combine embeddings with word classes\n",
        "    indexed_embeddings = {}\n",
        "    for i, (pos) in enumerate(word_classes):\n",
        "      indexed_embeddings[i] = {'embedding': embeddings[i], 'word_class': pos}\n",
        "\n",
        "    return indexed_embeddings\n"
      ],
      "metadata": {
        "id": "G02qAMiW8HjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grab_list2use(word_metric):\n",
        "    if metric == \"content\":\n",
        "        list2use = ['ADJ', 'ADV', 'INTJ', 'NOUN', 'PROPN']\n",
        "    elif metric == \"function\":\n",
        "        list2use = ['CONJ', 'CCONJ', 'DET', 'NUM', 'PART', 'PRON', 'SCONJ', 'ADP']\n",
        "    elif metric == \"predicate\":\n",
        "        list2use = ['AUX', 'VERB']\n",
        "    elif metric == \"all\":\n",
        "        list2use = ['ADJ', 'ADV', 'INTJ', 'NOUN', 'PROPN', 'CONJ', 'CCONJ', 'DET', 'NUM', 'PART', 'PRON', 'SCONJ', 'ADP', 'AUX', 'VERB']\n",
        "    return list2use"
      ],
      "metadata": {
        "id": "ukc7-XH8BiBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_by_word_class(combined_embeddings, word_metric, sim_metric):\n",
        "\n",
        "    list2use = grab_list2use(word_metric)\n",
        "    # Use only metric of interest\n",
        "    filtered_embeddings = [item['embedding'].cpu().numpy() for item in combined_embeddings if item['word_class'][1] in list2use]\n",
        "    #print(f\"Metric {metric}: {len(filtered_embeddings)}\")\n",
        "\n",
        "    similarity_embeddings = np.around(1-pairwise_distances(filtered_embeddings, filtered_embeddings, metric=sim_metric), 1)\n",
        "\n",
        "    lower_triangle = np.tril(similarity_embeddings)\n",
        "\n",
        "    similarity_vector = list(lower_triangle[np.tril_indices_from(lower_triangle)])\n",
        "\n",
        "    return similarity_embeddings, similarity_vector\n"
      ],
      "metadata": {
        "id": "UcaTZhRb8WwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity_entropy(similarity_vector):\n",
        "\n",
        "    # higher entropy represents more diversity of context\n",
        "    # computed as a function of n of instances of similarity values distributed between -1 and 1\n",
        "    # where 1 is complete similarity (0 degrees) and 0 non similarity (90 degrees), and -1 is complete dissimilarity (180 degrees)\n",
        "    # we will round to the 1st decimal so we can only have instances of 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1\n",
        "\n",
        "    length_of_vector = len(similarity_vector)\n",
        "\n",
        "    # get median of distribution\n",
        "    median_similarity = np.median(similarity_vector)\n",
        "\n",
        "    # get standard deviation\n",
        "    std_similarity = np.std(similarity_vector)\n",
        "\n",
        "    # create empty dict\n",
        "    counts = {value: 0 for value in similarity_vector}\n",
        "\n",
        "    # count number of instances of values 0 to 10 in the similarity vector\n",
        "    for value in similarity_vector: #sim_vector_by10:\n",
        "        if value in counts:\n",
        "            counts[value] += 1\n",
        "\n",
        "    # compute probabilities for each instance in your given similarity vector (p(i) = count_i/n) where n is the total number of similarity comparisons\n",
        "    # we get the n from the length of the similarity vector\n",
        "    probabilities = {}\n",
        "\n",
        "    for value, count in counts.items():\n",
        "        probabilities[value] = count/length_of_vector\n",
        "\n",
        "    entropy_probabilities = {}\n",
        "\n",
        "    for value, probability in probabilities.items():\n",
        "        if probability == 0:\n",
        "            entropy_probabilities[value] = 0\n",
        "        else:\n",
        "            entropy_probabilities[value] = -1*(math.log(probability)*probability)\n",
        "\n",
        "    final_values_to_compute_entropy_on = list(entropy_probabilities.values())\n",
        "\n",
        "    entropy_value = sum(final_values_to_compute_entropy_on)\n",
        "\n",
        "    return entropy_value, median_similarity, std_similarity"
      ],
      "metadata": {
        "id": "Dhghbll2AREL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run analysis"
      ],
      "metadata": {
        "id": "5SJUTcWqAicT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "narrative_df = pd.DataFrame({ \"narratives_clean\" : None,\n",
        "                            \"word_count\" : None},\n",
        "                            index=range(len(plot_summaries)))\n",
        "\n",
        "\n",
        "for i in range(len(plot_summaries)):\n",
        "  word_count = count_words(plot_summaries[i])\n",
        "  narrative_df.loc[i, 'narratives_clean'] = plot_summaries[i]\n",
        "  narrative_df.loc[i, 'word_count'] = word_count"
      ],
      "metadata": {
        "id": "SMEMmcUjCV97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# subset df for word counts to be within token window (< 512 since we're comparing with BERT, even though GPT can take up to 1024)\n",
        "# ~750 words = 1000 tokens (ref: https://news.ycombinator.com/item?id=35841781)\n",
        "\n",
        "narrative_df = narrative_df[narrative_df[\"word_count\"].between(100, 300)]\n",
        "len(narrative_df)"
      ],
      "metadata": {
        "id": "IdUN80SGsyEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "narrative_sampled_df = narrative_df.sample(n=300, random_state = 999)"
      ],
      "metadata": {
        "id": "Jo13HLei-DTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "narrative_sampled_df.head()"
      ],
      "metadata": {
        "id": "M6AmEpR3FUJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_metric = \"cosine\"\n",
        "metric = \"all\"\n",
        "lm_model = \"gpt\"\n",
        "\n",
        "# Initialize lists to store per-narrative results\n",
        "gpt_entropy = []\n",
        "gpt_median = []\n",
        "gpt_std = []\n",
        "gpt_count = []\n",
        "gpt_sim = []\n",
        "\n",
        "combined_embeddings = []\n",
        "layer_labels = []\n",
        "\n",
        "for i in range(len(narrative_sampled_df)):\n",
        "\n",
        "    shannon_entropy_row = {}\n",
        "    count_row = {}\n",
        "    sim_row = {}\n",
        "    median_similarity_dist = {}\n",
        "    std_similarity_dist = {}\n",
        "\n",
        "    # grab narrative you want to use and its respective word count\n",
        "    narrative = narrative_sampled_df['narratives_clean'].iat[i]\n",
        "    word_count = narrative_sampled_df['word_count'].iat[i]\n",
        "\n",
        "    # grab word embeddings for every word in the narrative, for every layer\n",
        "    all_layer_embeddings = compute_word_embedding(narrative, lm_model)\n",
        "\n",
        "    # tag each word with it's respective word class (based on the Space tokenizer)\n",
        "    word_classes = tag_words(narrative)\n",
        "\n",
        "    # loop across input + 12 (13) layers of the model and store each embedding\n",
        "    # compute similarity between each word embedding for each layer\n",
        "    # compute entropy on the resulting word level similarity distribution\n",
        "\n",
        "    for layer_index in range(13):\n",
        "        layername = f'layer_{layer_index}'\n",
        "        column_name = f'entropy_{layername}'\n",
        "\n",
        "        # grab the embeddings for the needed layer\n",
        "        embeddings = grab_embeddings_by_layer(all_layer_embeddings, layer_index, word_classes)\n",
        "\n",
        "        # store out embeddings for later t-SNE plots (layer_labels also stores which layer these embeddings correspond to)\n",
        "        combined_embeddings.append(embeddings)\n",
        "        layer_labels.extend([layer_index] * len(embeddings))\n",
        "\n",
        "        # compute similarity on each word embedding, word by word\n",
        "        # before we do that we need to pass the embeddings as a list\n",
        "        embed_list = list(embeddings.values())\n",
        "        similarity_df, similarity_vector = similarity_by_word_class(embed_list, metric, sim_metric)\n",
        "\n",
        "        # save out our word x word similairty matrix for latter plotting\n",
        "        sim_row[layername] = similarity_df\n",
        "\n",
        "\n",
        "        # on the lower triangle of the similarity distribution, compute shannon's entropy\n",
        "        # also store out the median and the standard deviation of the distribution\n",
        "        shannon, median, std = compute_similarity_entropy(similarity_vector)\n",
        "        shannon_entropy_row[column_name] = shannon\n",
        "        median_similarity_dist[column_name] = median\n",
        "        std_similarity_dist[column_name] = std\n",
        "\n",
        "        # store out how many unmique instances they were of each similarity value in the vector\n",
        "        # for if we want to plot the distribution later on\n",
        "        unique, counts = np.unique(similarity_vector, return_counts=True)\n",
        "        count_row[layername] = dict(zip(unique, counts))\n",
        "\n",
        "    # append per-narrative results\n",
        "    gpt_entropy.append(shannon_entropy_row)\n",
        "    gpt_median.append(median_similarity_dist)\n",
        "    gpt_std.append(std_similarity_dist)\n",
        "    gpt_count.append(count_row)\n",
        "    gpt_sim.append(sim_row)\n",
        "\n",
        "\n",
        "gpt_entropy_df = pd.DataFrame(gpt_entropy)\n",
        "gpt_median_df = pd.DataFrame(gpt_median)\n",
        "gpt_std_df = pd.DataFrame(gpt_std)\n"
      ],
      "metadata": {
        "id": "1B0BuSxrBKxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(gpt_entropy_df)):\n",
        "    plt.plot(range(gpt_entropy_df.shape[1]), gpt_entropy_df.iloc[i])\n",
        "\n",
        "plt.xlabel('Model Layers')\n",
        "plt.ylabel('Entropy')\n",
        "plt.title(\"Shannon's entropy as it changes over layers of the GPT model\")\n",
        "\n",
        "# save out\n",
        "# plt.savefig(\"gpt_entropy.png\", dpi = 600)\n",
        "# files.download(\"gpt_entropy.png\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k2ofstqsBAFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(gpt_median_df)):\n",
        "    plt.plot(range(gpt_median_df.shape[1]), gpt_median_df.iloc[i])\n",
        "\n",
        "plt.xlabel('Model Layers')\n",
        "plt.ylabel('Median')\n",
        "plt.title(\"Median of the similarity distribution as it changes over layers of the GPT model\")\n",
        "\n",
        "# save out\n",
        "# plt.savefig(\"gpt_median.png\", dpi = 600)\n",
        "# files.download(\"gpt_median.png\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WaRbXbobh3RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(gpt_std_df)):\n",
        "    plt.plot(range(gpt_std_df.shape[1]), gpt_std_df.iloc[i])\n",
        "\n",
        "plt.xlabel('Model Layers')\n",
        "plt.ylabel('Standard Deviation')\n",
        "plt.title(\"Standard deviation of the similarity distribution as it changes over layers of the GPT model\")\n",
        "\n",
        "# save out\n",
        "# plt.savefig(\"gpt_std.png\", dpi = 600)\n",
        "# files.download(\"gpt_std.png\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CCi2zOghuKiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = 13\n",
        "n_cols = 4\n",
        "n_rows = (n_layers + n_cols - 1) // n_cols\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for layer_index in range(n_layers):\n",
        "    #plt.figure()  # create a new figure for each layer\n",
        "    ax = axes[layer_index]\n",
        "\n",
        "    layername = f'layer_{layer_index}'\n",
        "\n",
        "    for i in range(len(gpt_count)):\n",
        "      #ax = axes[layer_index, layer_index]\n",
        "      ax.plot(\n",
        "            list(gpt_count[i][layername].keys()),\n",
        "            list(gpt_count[i][layername].values())#,\n",
        "            #marker='D'\n",
        "        )\n",
        "      ax.set_xlabel('Similarity Value')\n",
        "      ax.set_ylabel('Count')\n",
        "      ax.set_title(f'Layer {layer_index}')\n",
        "\n",
        "for j in range(n_layers, len(axes)):\n",
        "  fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "# save out\n",
        "# plt.savefig(\"gpt_density.png\", dpi = 600)\n",
        "# files.download(\"gpt_density.png\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C206aAshoki5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://matplotlib.org/stable/gallery/color/named_colors.html\n",
        "from sklearn.metrics import r2_score # https://data36.com/linear-regression-in-python-numpy-polyfit/\n",
        "\n",
        "n_layers = 13\n",
        "n_cols = 4\n",
        "n_rows = (n_layers + n_cols - 1) // n_cols\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for layer_index in range(n_layers):\n",
        "\n",
        "    layername = f'layer_{layer_index}'\n",
        "    column_name = f'entropy_{layername}'\n",
        "\n",
        "    ax = axes[layer_index]\n",
        "\n",
        "    layername = f'layer_{layer_index}'\n",
        "\n",
        "    x = np.array(narrative_sampled_df['word_count']).astype(float)\n",
        "    y = np.array(gpt_entropy_df[column_name]).astype(float)\n",
        "\n",
        "    # get quantile value\n",
        "    # y_quantile = np.quantile(y, 0.75)\n",
        "    max_val = max(y) - 0.04\n",
        "\n",
        "    # create line of best fit us np.ployfit\n",
        "    # https://www.statology.org/line-of-best-fit-python/\n",
        "\n",
        "    a, b = np.polyfit(x, y, 1)\n",
        "    y_pred = a * x + b\n",
        "    r2 = np.round(r2_score(y, y_pred), 2)\n",
        "    ax.scatter(x, y, color = 'plum')\n",
        "    ax.plot(x, a*x+b, color = 'indigo')\n",
        "    ax.text(x[10], max_val, f\"r = {r2}\", size=12)\n",
        "\n",
        "    ax.set_xlabel('Word count')\n",
        "    ax.set_ylabel('Entropy')\n",
        "    ax.set_title(f'Layer {layer_index}')\n",
        "\n",
        "for j in range(n_layers, len(axes)):\n",
        "  fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "# save out\n",
        "# plt.savefig(\"gpt_word2entropy.png\", dpi = 600)\n",
        "# files.download(\"gpt_word2entropy.png\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8sT3dOsBkxL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
        "# https://distill.pub/2016/misread-tsne/\n",
        "# NB: t-SNE embeddings try to find clustering patterns\n",
        "# Why not UMAP? https://datascience.stackexchange.com/questions/121310/dimension-reduction-of-word-embeddings-pca-vs-tsne\n",
        "# https://www.datacamp.com/tutorial/introduction-t-sne\n",
        "#\n",
        "#\n",
        "# Plot embeddings of example narrative 1\n",
        "\n",
        "# index = 1\n",
        "\n",
        "for index in range(10):\n",
        "\n",
        "  tsne = TSNE(n_components=2, random_state=999)\n",
        "\n",
        "  gpt_combined_embeddings = []\n",
        "  gpt_layer_labels = []\n",
        "\n",
        "  narrative = narrative_sampled_df['narratives_clean'].iat[index]\n",
        "\n",
        "  all_layer_embeddings = compute_word_embedding(narrative, 'gpt')\n",
        "  word_classes = tag_words(narrative)\n",
        "\n",
        "  list2use = grab_list2use(\"all\")\n",
        "\n",
        "  for layer_index in range(13):\n",
        "    embeddings = grab_embeddings_by_layer(all_layer_embeddings, layer_index, word_classes)\n",
        "    embed_list = list(embeddings.values())\n",
        "    filtered_embeddings = [item['embedding'].cpu().numpy() for item in embed_list if item['word_class'][1] in list2use]\n",
        "\n",
        "    for _ in filtered_embeddings:\n",
        "        gpt_layer_labels.append(layer_index)\n",
        "\n",
        "    gpt_combined_embeddings.append(filtered_embeddings)\n",
        "\n",
        "  # Combine all embeddings into a single array\n",
        "  gpt_combined_embeddings = np.vstack(gpt_combined_embeddings)\n",
        "\n",
        "  # Fit and transform the combined embeddings\n",
        "  embeddings_2d = tsne.fit_transform(gpt_combined_embeddings)\n",
        "\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=gpt_layer_labels, cmap='tab20b')\n",
        "\n",
        "  legend1 = plt.legend(*scatter.legend_elements(), title=\"Layers\")\n",
        "  plt.gca().add_artist(legend1)\n",
        "\n",
        "  narrative_num = index + 1\n",
        "  plt.title(f\"GPT word embeddings from Narrative {narrative_num}\")\n",
        "  plt.xlabel('First Dimension (t-SNE)')\n",
        "  plt.ylabel('Second Dimension (t-SNE)')\n",
        "\n",
        "  # save out\n",
        "  # plt.savefig(f\"tsne_{index}.png\", dpi = 600)\n",
        "  # files.download(f\"tsne_{index}.png\")\n",
        "\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "b_PU7oEdxBhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_1 = gpt_sim[0]\n",
        "words = narrative_sampled_df['narratives_clean'].iat[0].split()\n",
        "for layer_index in range(6):\n",
        "    layername = f'layer_{layer_index}'\n",
        "    similarity_embeddings = plot_1[layername] # Get the similarity matrix for the current layer\n",
        "\n",
        "    plt.figure(figsize=(8, 6)) # Create a new figure for each heatmap\n",
        "\n",
        "    sns.heatmap(similarity_embeddings,\n",
        "                annot=False,\n",
        "                cmap=\"mako\",\n",
        "                yticklabels=False,\n",
        "                xticklabels=False)\n",
        "    #plt.axis('square')\n",
        "    plt.title(f\"Layer {layer_index} Similarity Matrix\")\n",
        "    # save out\n",
        "    # plt.savefig(f\"similarity_{layer_index}_0.png\", dpi = 600)\n",
        "    # files.download(f\"similarity_{layer_index}_0.png\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Y4xAXxXpq8jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_1 = gpt_sim[0]\n",
        "words = narrative_sampled_df['narratives_clean'].iat[0].split()\n",
        "for layer_index in range(6, 13):\n",
        "    layername = f'layer_{layer_index}'\n",
        "    similarity_embeddings = plot_1[layername] # Get the similarity matrix for the current layer\n",
        "\n",
        "    plt.figure(figsize=(8, 6)) # Create a new figure for each heatmap\n",
        "\n",
        "    sns.heatmap(similarity_embeddings,\n",
        "                annot=False,\n",
        "                cmap=\"mako\",\n",
        "                yticklabels=False,\n",
        "                xticklabels=False)\n",
        "    #plt.axis('square')\n",
        "    plt.title(f\"Layer {layer_index} Similarity Matrix\")\n",
        "    # save out\n",
        "    # plt.savefig(f\"similarity_{layer_index}_0.png\", dpi = 600)\n",
        "    # files.download(f\"similarity_{layer_index}_0.png\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GPEJML8-_T1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find 'elbow' words in layer 12\n",
        "\n",
        "words = tag_words(str(narrative_sampled_df['narratives_clean'].iat[0]))\n",
        "list2use = grab_list2use(\"all\")\n",
        "\n",
        "words2check = []\n",
        "for word in words:\n",
        "  if word[1] in list2use:\n",
        "    words2check.append(word[0])\n",
        "\n",
        "layer_12_similarity = plot_1['layer_12']\n",
        "elbow_indices = []\n",
        "for row in range(len(layer_12_similarity)):\n",
        "  if sum(layer_12_similarity[row]) < 0:\n",
        "    elbow_indices.append(row)\n",
        "\n",
        "if len(words2check) == len(layer_12_similarity):\n",
        "  for index in elbow_indices:\n",
        "    print(f\"{words2check[index]} at index {index}\")"
      ],
      "metadata": {
        "id": "suQ3oIP2UtBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_1 = gpt_sim[1]\n",
        "words = narrative_sampled_df['narratives_clean'].iat[1].split()\n",
        "for layer_index in range(6):\n",
        "    layername = f'layer_{layer_index}'\n",
        "    similarity_embeddings = plot_1[layername] # Get the similarity matrix for the current layer\n",
        "\n",
        "    plt.figure(figsize=(8, 6)) # Create a new figure for each heatmap\n",
        "\n",
        "    sns.heatmap(similarity_embeddings,\n",
        "                annot=False,\n",
        "                cmap=\"mako\",\n",
        "                yticklabels=False,\n",
        "                xticklabels=False)\n",
        "    #plt.axis('square')\n",
        "    plt.title(f\"Layer {layer_index} Similarity Matrix\")\n",
        "    # save out\n",
        "    # plt.savefig(f\"similarity_{layer_index}_1.png\", dpi = 600)\n",
        "    # files.download(f\"similarity_{layer_index}_1.png\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wuL-JQxP9R4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_1 = gpt_sim[1]\n",
        "words = narrative_sampled_df['narratives_clean'].iat[1].split()\n",
        "for layer_index in range(6, 13):\n",
        "    layername = f'layer_{layer_index}'\n",
        "    similarity_embeddings = plot_1[layername] # Get the similarity matrix for the current layer\n",
        "\n",
        "    plt.figure(figsize=(8, 6)) # Create a new figure for each heatmap\n",
        "\n",
        "    sns.heatmap(similarity_embeddings,\n",
        "                annot=False,\n",
        "                cmap=\"mako\",\n",
        "                yticklabels=False,\n",
        "                xticklabels=False)\n",
        "    #plt.axis('square')\n",
        "    plt.title(f\"Layer {layer_index} Similarity Matrix\")\n",
        "    # save out\n",
        "    # plt.savefig(f\"similarity_{layer_index}_1.png\", dpi = 600)\n",
        "    # files.download(f\"similarity_{layer_index}_1.png\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wMsGFgB6_wDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_1 = gpt_sim[2]\n",
        "words = narrative_sampled_df['narratives_clean'].iat[2].split()\n",
        "for layer_index in range(6):\n",
        "    layername = f'layer_{layer_index}'\n",
        "    similarity_embeddings = plot_1[layername] # Get the similarity matrix for the current layer\n",
        "\n",
        "    plt.figure(figsize=(8, 6)) # Create a new figure for each heatmap\n",
        "\n",
        "    sns.heatmap(similarity_embeddings,\n",
        "                annot=False,\n",
        "                cmap=\"mako\",\n",
        "                yticklabels=False,\n",
        "                xticklabels=False)\n",
        "    #plt.axis('square')\n",
        "    plt.title(f\"Layer {layer_index} Similarity Matrix\")\n",
        "    # save out\n",
        "    # plt.savefig(f\"similarity_{layer_index}_2.png\", dpi = 600)\n",
        "    # files.download(f\"similarity_{layer_index}_2.png\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Vof1Axpj9Th6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_1 = gpt_sim[2]\n",
        "words = narrative_sampled_df['narratives_clean'].iat[2].split()\n",
        "for layer_index in range(6, 13):\n",
        "    layername = f'layer_{layer_index}'\n",
        "    similarity_embeddings = plot_1[layername] # Get the similarity matrix for the current layer\n",
        "\n",
        "    plt.figure(figsize=(8, 6)) # Create a new figure for each heatmap\n",
        "\n",
        "    sns.heatmap(similarity_embeddings,\n",
        "                annot=False,\n",
        "                cmap=\"mako\",\n",
        "                yticklabels=False,\n",
        "                xticklabels=False)\n",
        "    #plt.axis('square')\n",
        "    plt.title(f\"Layer {layer_index} Similarity Matrix\")\n",
        "    # save out\n",
        "    # plt.savefig(f\"similarity_{layer_index}_2.png\", dpi = 600)\n",
        "    # files.download(f\"similarity_{layer_index}_2.png\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fWgVaGfR_334"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anistropy and its relationship to interpretable meaning: \"signal words\"**\n",
        "\n",
        "By looking at word by word embedding cosine similarity GPT's layers, and interesting patter emerges. In the last layer of the model, the majority of words reach almsot perfect cosine similarity (in line with previous findings of the embedding space becoming increasingly anistropic).\n",
        "\n",
        "If it were the case that this is simply an afterfact of all the words all now representing some kind of 'global' meaning related to the given context window (in this case our narrative), then why do we see these 'grids' form of single words that suddenly have perfect cosine dissimilarity (180 degrees of angle)?\n",
        "\n",
        "NB: Note that this is the only layer for which we see negative values in the similarity distribution.\n",
        "\n",
        "I term them 'signal' words, in that it is possible they containing signalling information to the model as moves from embedding to embedding (or they may just be noise, but the fact it's a repeated patters that occurs suggests otherwise). To examine if there are any co-occuring patterns regarding these 'elbow' words, I will count for each narrative the word class that these words belong to.\n",
        "\n",
        "We will see below, that the majority of them belong to noun and verb classes."
      ],
      "metadata": {
        "id": "QUUXyR_q9O2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find 'signal' words in layer 12\n",
        "\n",
        "all_counts = []\n",
        "list2use = grab_list2use(\"all\")\n",
        "\n",
        "for index in range(len(narrative_sampled_df)):\n",
        "  words2check = []\n",
        "  plots = gpt_sim[index]\n",
        "  words = tag_words(str(narrative_sampled_df['narratives_clean'].iat[index]))\n",
        "  pos_counts = {tag: 0 for tag in list2use}\n",
        "\n",
        "  words2check = []\n",
        "  for word in words:\n",
        "    if word[1] in list2use:\n",
        "      words2check.append(word[1])\n",
        "\n",
        "  layer_12_similarity = plots['layer_12']\n",
        "  elbow_indices = []\n",
        "  for row in range(len(layer_12_similarity)):\n",
        "    if sum(layer_12_similarity[row]) < 0:\n",
        "      value = words2check[row]\n",
        "      if value in list2use:\n",
        "          pos_counts[value] += 1\n",
        "  all_counts.append(pos_counts)"
      ],
      "metadata": {
        "id": "wBmpKARKXqXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_counts = {tag: 0 for tag in list2use}\n",
        "\n",
        "for counts_dict in all_counts:\n",
        "    for pos, count in counts_dict.items():\n",
        "        total_counts[pos] += count\n",
        "\n",
        "# https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
        "total_counts_sorted = dict(sorted(total_counts.items(), key=lambda item: item[1]))\n",
        "\n",
        "plt.plot(total_counts_sorted.keys(), total_counts_sorted.values(), marker='D')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.xlabel('Part of Speech')\n",
        "plt.ylabel('Count')\n",
        "plt.title(\"Distribution of Unique Values\")\n",
        "\n",
        "# save out\n",
        "#plt.savefig(f\"elbow_dist.png\",dpi = 600)\n",
        "#files.download(f\"elbow_dist.png\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "wtRQrdfIc7D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_percs_sorted = [count / sum(list(total_counts_sorted.values())) for count in list(total_counts_sorted.values())]\n",
        "\n",
        "plt.plot(total_counts_sorted.keys(), total_percs_sorted, marker='D')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.xlabel('Part of Speech')\n",
        "plt.ylabel('Percentage')\n",
        "plt.title(\"Percentage of Unique Values\")\n",
        "\n",
        "# save out\n",
        "#plt.savefig(f\"elbow_perc.png\",dpi = 600)\n",
        "#files.download(f\"elbow_perc.png\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mFGoiGyvgO71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run all of the above analysis, but now for BERT**"
      ],
      "metadata": {
        "id": "IB6DuokVdpOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sim_metric = \"cosine\"\n",
        "metric = \"all\"\n",
        "lm_model = \"bert\"\n",
        "\n",
        "# Initialize lists to store per-narrative results\n",
        "bert_entropy = []\n",
        "bert_median = []\n",
        "bert_std = []\n",
        "bert_count = []\n",
        "bert_sim = []\n",
        "\n",
        "combined_embeddings = []\n",
        "layer_labels = []\n",
        "\n",
        "for i in range(len(narrative_sampled_df)):\n",
        "\n",
        "    shannon_entropy_row = {}\n",
        "    count_row = {}\n",
        "    sim_row = {}\n",
        "    median_similarity_dist = {}\n",
        "    std_similarity_dist = {}\n",
        "\n",
        "    # grab narrative you want to use and its respective word count\n",
        "    narrative = narrative_sampled_df['narratives_clean'].iat[i]\n",
        "    word_count = narrative_sampled_df['word_count'].iat[i]\n",
        "\n",
        "    # grab word embeddings for every word in the narrative, for every layer\n",
        "    all_layer_embeddings = compute_word_embedding(narrative, lm_model)\n",
        "\n",
        "    # tag each word with it's respective word class (based on the Space tokenizer)\n",
        "    word_classes = tag_words(narrative)\n",
        "\n",
        "    # loop across input + 12 (13) layers of the model and store each embedding\n",
        "    # compute similarity between each word embedding for each layer\n",
        "    # compute entropy on the resulting word level similarity distribution\n",
        "\n",
        "    for layer_index in range(13):\n",
        "        layername = f'layer_{layer_index}'\n",
        "        column_name = f'entropy_{layername}'\n",
        "\n",
        "        # grab the embeddings for the needed layer\n",
        "        embeddings = grab_embeddings_by_layer(all_layer_embeddings, layer_index, word_classes)\n",
        "\n",
        "        # store out embeddings for later t-SNE plots (layer_labels also stores which layer these embeddings correspond to)\n",
        "        combined_embeddings.append(embeddings)\n",
        "        layer_labels.extend([layer_index] * len(embeddings))\n",
        "\n",
        "        # compute similarity on each word embedding, word by word\n",
        "        # before we do that we need to pass the embeddings as a list\n",
        "        embed_list = list(embeddings.values())\n",
        "        similarity_df, similarity_vector = similarity_by_word_class(embed_list, metric, sim_metric)\n",
        "\n",
        "        # save out our word x word similairty matrix for latter plotting\n",
        "        sim_row[layername] = similarity_df\n",
        "\n",
        "\n",
        "        # on the lower triangle of the similarity distribution, compute shannon's entropy\n",
        "        # also store out the median and the standard deviation of the distribution\n",
        "        shannon, median, std = compute_similarity_entropy(similarity_vector)\n",
        "        shannon_entropy_row[column_name] = shannon\n",
        "        median_similarity_dist[column_name] = median\n",
        "        std_similarity_dist[column_name] = std\n",
        "\n",
        "        # store out how many unmique instances they were of each similarity value in the vector\n",
        "        # for if we want to plot the distribution later on\n",
        "        unique, counts = np.unique(similarity_vector, return_counts=True)\n",
        "        count_row[layername] = dict(zip(unique, counts))\n",
        "\n",
        "    # append per-narrative results\n",
        "    bert_entropy.append(shannon_entropy_row)\n",
        "    bert_median.append(median_similarity_dist)\n",
        "    bert_std.append(std_similarity_dist)\n",
        "    bert_count.append(count_row)\n",
        "    bert_sim.append(sim_row)\n",
        "\n",
        "\n",
        "bert_entropy_df = pd.DataFrame(bert_entropy)\n",
        "bert_median_df = pd.DataFrame(bert_median)\n",
        "bert_std_df = pd.DataFrame(bert_std)\n"
      ],
      "metadata": {
        "id": "HFsTXUq0doH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(bert_entropy_df)):\n",
        "    plt.plot(range(bert_entropy_df.shape[1]), bert_entropy_df.iloc[i])\n",
        "\n",
        "plt.xlabel('Model Layers')\n",
        "plt.ylabel('Entropy')\n",
        "plt.title(\"Shannon's entropy as it changes over layers of the BERT model\")\n",
        "\n",
        "# save out\n",
        "# plt.savefig(\"bert_entropy.png\", dpi = 600)\n",
        "# files.download(\"bert_entropy.png\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yTouiWKvd7NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(bert_median_df)):\n",
        "    plt.plot(range(bert_median_df.shape[1]), bert_median_df.iloc[i])\n",
        "\n",
        "plt.xlabel('Model Layers')\n",
        "plt.ylabel('Median')\n",
        "plt.title(\"Median of the similarity distribution as it changes over layers of the BERT model\")\n",
        "\n",
        "# save out\n",
        "# plt.savefig(\"bert_median.png\", dpi = 600)\n",
        "# files.download(\"bert_median.png\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4S8gl3Wxh_Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(bert_std_df)):\n",
        "    plt.plot(range(bert_std_df.shape[1]), bert_std_df.iloc[i])\n",
        "\n",
        "plt.xlabel('Model Layers')\n",
        "plt.ylabel('Standard Deviation')\n",
        "plt.title(\"Standard deviation of the similarity distribution as it changes over layers of the BERT model\")\n",
        "\n",
        "# save out\n",
        "# plt.savefig(\"bert_std.png\", dpi = 600)\n",
        "# files.download(\"bert_std.png\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EuG6bZVqiHDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = 13\n",
        "n_cols = 4\n",
        "n_rows = (n_layers + n_cols - 1) // n_cols\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for layer_index in range(n_layers):\n",
        "    #plt.figure()  # create a new figure for each layer\n",
        "    ax = axes[layer_index]\n",
        "\n",
        "    layername = f'layer_{layer_index}'\n",
        "\n",
        "    for i in range(len(bert_count)):\n",
        "      #ax = axes[layer_index, layer_index]\n",
        "      ax.plot(\n",
        "            list(bert_count[i][layername].keys()),\n",
        "            list(bert_count[i][layername].values())#,\n",
        "            #marker='D'\n",
        "        )\n",
        "      ax.set_xlabel('Similarity Value')\n",
        "      ax.set_ylabel('Count')\n",
        "      ax.set_title(f'Layer {layer_index}')\n",
        "\n",
        "for j in range(n_layers, len(axes)):\n",
        "  fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "# save out\n",
        "# plt.savefig(\"bert_density.png\", dpi = 600)\n",
        "# files.download(\"bert_density.png\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B94UNlY0iPJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://matplotlib.org/stable/gallery/color/named_colors.html\n",
        "from sklearn.metrics import r2_score # https://data36.com/linear-regression-in-python-numpy-polyfit/\n",
        "\n",
        "n_layers = 13\n",
        "n_cols = 4\n",
        "n_rows = (n_layers + n_cols - 1) // n_cols\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for layer_index in range(n_layers):\n",
        "\n",
        "    layername = f'layer_{layer_index}'\n",
        "    column_name = f'entropy_{layername}'\n",
        "\n",
        "    ax = axes[layer_index]\n",
        "\n",
        "    layername = f'layer_{layer_index}'\n",
        "\n",
        "    x = np.array(narrative_sampled_df['word_count']).astype(float)\n",
        "    y = np.array(bert_entropy_df[column_name]).astype(float)\n",
        "\n",
        "    # get quantile value\n",
        "    # y_quantile = np.quantile(y, 0.75)\n",
        "    max_val = max(y) - 0.04\n",
        "\n",
        "    # create line of best fit us np.ployfit\n",
        "    # https://www.statology.org/line-of-best-fit-python/\n",
        "\n",
        "    a, b = np.polyfit(x, y, 1)\n",
        "    y_pred = a * x + b\n",
        "    r2 = np.round(r2_score(y, y_pred), 2)\n",
        "    ax.scatter(x, y, color = 'lightskyblue')\n",
        "    ax.plot(x, a*x+b, color = 'steelblue')\n",
        "    ax.text(110, max_val, f\"r = {r2}\", size=12)\n",
        "\n",
        "    ax.set_xlabel('Word count')\n",
        "    ax.set_ylabel('Entropy')\n",
        "    ax.set_title(f'Layer {layer_index}')\n",
        "\n",
        "for j in range(n_layers, len(axes)):\n",
        "  fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "# save out\n",
        "# plt.savefig(\"bert_word2entropy.png\", dpi = 600)\n",
        "# files.download(\"bert_word2entropy.png\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yx01naNCia7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
        "# https://distill.pub/2016/misread-tsne/\n",
        "# NB: t-SNE embeddings try to find clustering patterns\n",
        "# Why not UMAP? https://datascience.stackexchange.com/questions/121310/dimension-reduction-of-word-embeddings-pca-vs-tsne\n",
        "\n",
        "# Plot embeddings of example narrative 1\n",
        "\n",
        "# index = 1\n",
        "\n",
        "for index in range(10):\n",
        "\n",
        "  tsne = TSNE(n_components=2, random_state=999)\n",
        "\n",
        "  bert_combined_embeddings = []\n",
        "  bert_layer_labels = []\n",
        "\n",
        "  narrative = narrative_sampled_df['narratives_clean'].iat[index]\n",
        "\n",
        "  all_layer_embeddings = compute_word_embedding(narrative, 'bert')\n",
        "  word_classes = tag_words(narrative)\n",
        "\n",
        "  list2use = grab_list2use(\"all\")\n",
        "\n",
        "  for layer_index in range(13):\n",
        "    embeddings = grab_embeddings_by_layer(all_layer_embeddings, layer_index, word_classes)\n",
        "    embed_list = list(embeddings.values())\n",
        "    filtered_embeddings = [item['embedding'].cpu().numpy() for item in embed_list if item['word_class'][1] in list2use]\n",
        "\n",
        "    for _ in filtered_embeddings:\n",
        "        bert_layer_labels.append(layer_index)\n",
        "\n",
        "    bert_combined_embeddings.append(filtered_embeddings)\n",
        "\n",
        "  # Combine all embeddings into a single array\n",
        "  bert_combined_embeddings = np.vstack(bert_combined_embeddings)\n",
        "\n",
        "  # Fit and transform the combined embeddings\n",
        "  embeddings_2d = tsne.fit_transform(bert_combined_embeddings)\n",
        "\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=bert_layer_labels, cmap='tab20c')\n",
        "\n",
        "  legend1 = plt.legend(*scatter.legend_elements(), title=\"Layers\")\n",
        "  plt.gca().add_artist(legend1)\n",
        "\n",
        "  narrative_num = index + 1\n",
        "  plt.title(f\"Bert word embeddings from Narrative {narrative_num}\")\n",
        "  plt.xlabel('First Dimension (t-SNE)')\n",
        "  plt.ylabel('Second Dimension (t-SNE)')\n",
        "\n",
        "  # save out\n",
        "  # plt.savefig(f\"bert_tsne_{index}.png\", dpi = 600)\n",
        "  # files.download(f\"bert_tsne_{index}.png\")\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "JJb2FbrKijgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_1 = bert_sim[0]\n",
        "words = narrative_sampled_df['narratives_clean'].iat[0].split()\n",
        "for layer_index in range(6):\n",
        "    layername = f'layer_{layer_index}'\n",
        "    similarity_embeddings = plot_1[layername] # Get the similarity matrix for the current layer\n",
        "\n",
        "    plt.figure(figsize=(8, 6)) # Create a new figure for each heatmap\n",
        "\n",
        "    sns.heatmap(similarity_embeddings,\n",
        "                annot=False,\n",
        "                cmap=\"mako\",\n",
        "                yticklabels=False,\n",
        "                xticklabels=False)\n",
        "    #plt.axis('square')\n",
        "    plt.title(f\"Layer {layer_index} Similarity Matrix\")\n",
        "    # save out\n",
        "    # plt.savefig(f\"bert_similarity_{layer_index}_0.png\", dpi = 600)\n",
        "    # files.download(f\"bert_similarity_{layer_index}_0.png\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xnT_OeCCL8lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_1 = bert_sim[0]\n",
        "words = narrative_sampled_df['narratives_clean'].iat[0].split()\n",
        "for layer_index in range(6, 13):\n",
        "    layername = f'layer_{layer_index}'\n",
        "    similarity_embeddings = plot_1[layername] # Get the similarity matrix for the current layer\n",
        "\n",
        "    plt.figure(figsize=(8, 6)) # Create a new figure for each heatmap\n",
        "\n",
        "    sns.heatmap(similarity_embeddings,\n",
        "                annot=False,\n",
        "                cmap=\"mako\",\n",
        "                yticklabels=False,\n",
        "                xticklabels=False)\n",
        "    #plt.axis('square')\n",
        "    plt.title(f\"Layer {layer_index} Similarity Matrix\")\n",
        "    # save out\n",
        "    # plt.savefig(f\"bert_similarity_{layer_index}_0.png\", dpi = 600)\n",
        "    # files.download(f\"bert_similarity_{layer_index}_0.png\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "eBPZUJLWRELs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_1 = bert_sim[1]\n",
        "words = narrative_sampled_df['narratives_clean'].iat[1].split()\n",
        "for layer_index in range(6):\n",
        "    layername = f'layer_{layer_index}'\n",
        "    similarity_embeddings = plot_1[layername] # Get the similarity matrix for the current layer\n",
        "\n",
        "    plt.figure(figsize=(8, 6)) # Create a new figure for each heatmap\n",
        "\n",
        "    sns.heatmap(similarity_embeddings,\n",
        "                annot=False,\n",
        "                cmap=\"mako\",\n",
        "                yticklabels=False,\n",
        "                xticklabels=False)\n",
        "    #plt.axis('square')\n",
        "    plt.title(f\"Layer {layer_index} Similarity Matrix\")\n",
        "    # save out\n",
        "    # plt.savefig(f\"bert_similarity_{layer_index}_1.png\", dpi = 600)\n",
        "    # files.download(f\"bert_similarity_{layer_index}_1.png\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rxq3jw5TRTpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_1 = bert_sim[1]\n",
        "words = narrative_sampled_df['narratives_clean'].iat[1].split()\n",
        "for layer_index in range(6, 13):\n",
        "    layername = f'layer_{layer_index}'\n",
        "    similarity_embeddings = plot_1[layername] # Get the similarity matrix for the current layer\n",
        "\n",
        "    plt.figure(figsize=(8, 6)) # Create a new figure for each heatmap\n",
        "\n",
        "    sns.heatmap(similarity_embeddings,\n",
        "                annot=False,\n",
        "                cmap=\"mako\",\n",
        "                yticklabels=False,\n",
        "                xticklabels=False)\n",
        "    #plt.axis('square')\n",
        "    plt.title(f\"Layer {layer_index} Similarity Matrix\")\n",
        "    # save out\n",
        "    # plt.savefig(f\"bert_similarity_{layer_index}_1.png\", dpi = 600)\n",
        "    # files.download(f\"bert_similarity_{layer_index}_1.png\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "UaoGhH6xRYnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_1 = bert_sim[2]\n",
        "words = narrative_sampled_df['narratives_clean'].iat[2].split()\n",
        "for layer_index in range(6):\n",
        "    layername = f'layer_{layer_index}'\n",
        "    similarity_embeddings = plot_1[layername] # Get the similarity matrix for the current layer\n",
        "\n",
        "    plt.figure(figsize=(8, 6)) # Create a new figure for each heatmap\n",
        "\n",
        "    sns.heatmap(similarity_embeddings,\n",
        "                annot=False,\n",
        "                cmap=\"mako\",\n",
        "                yticklabels=False,\n",
        "                xticklabels=False)\n",
        "    #plt.axis('square')\n",
        "    plt.title(f\"Layer {layer_index} Similarity Matrix\")\n",
        "    # save out\n",
        "    # plt.savefig(f\"bert_similarity_{layer_index}_2.png\", dpi = 600)\n",
        "    # files.download(f\"bert_similarity_{layer_index}_2.png\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "n1c0TqmYRgHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_1 = bert_sim[2]\n",
        "words = narrative_sampled_df['narratives_clean'].iat[2].split()\n",
        "for layer_index in range(6, 13):\n",
        "    layername = f'layer_{layer_index}'\n",
        "    similarity_embeddings = plot_1[layername] # Get the similarity matrix for the current layer\n",
        "\n",
        "    plt.figure(figsize=(8, 6)) # Create a new figure for each heatmap\n",
        "\n",
        "    sns.heatmap(similarity_embeddings,\n",
        "                annot=False,\n",
        "                cmap=\"mako\",\n",
        "                yticklabels=False,\n",
        "                xticklabels=False)\n",
        "    #plt.axis('square')\n",
        "    plt.title(f\"Layer {layer_index} Similarity Matrix\")\n",
        "    # save out\n",
        "    # plt.savefig(f\"bert_similarity_{layer_index}_2.png\", dpi = 600)\n",
        "    # files.download(f\"bert_similarity_{layer_index}_2.png\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "toNPEdE0Rmsv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}